# Complete Financial Data Engineering Pipeline

A comprehensive, production-ready data engineering pipeline for real-time financial data processing using modern technologies.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Streaming     â”‚    â”‚   Processing    â”‚    â”‚   Storage       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Yahoo Finance â”‚â”€â”€â”€â–¶â”‚ â€¢ Kafka         â”‚â”€â”€â”€â–¶â”‚ â€¢ PySpark       â”‚â”€â”€â”€â–¶â”‚ â€¢ S3/MinIO      â”‚
â”‚ â€¢ Real-time API â”‚    â”‚ â€¢ Real-time     â”‚    â”‚ â€¢ Structured    â”‚    â”‚ â€¢ Data Lake     â”‚
â”‚ â€¢ Historical    â”‚    â”‚ â€¢ Streaming     â”‚    â”‚   Streaming     â”‚    â”‚ â€¢ Parquet       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚                        â”‚
                                â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestration â”‚    â”‚   Analytics     â”‚    â”‚   Monitoring    â”‚    â”‚   Visualization â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Airflow       â”‚    â”‚ â€¢ dbt           â”‚    â”‚ â€¢ Grafana       â”‚    â”‚ â€¢ Streamlit     â”‚
â”‚ â€¢ DAGs          â”‚    â”‚ â€¢ Transformationsâ”‚   â”‚ â€¢ Prometheus    â”‚    â”‚ â€¢ Dashboards    â”‚
â”‚ â€¢ Scheduling    â”‚    â”‚ â€¢ Dimensional   â”‚    â”‚ â€¢ Alerts        â”‚    â”‚ â€¢ Real-time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Technology Stack

### Data Ingestion
- **Yahoo Finance API**: Real-time and historical financial data
- **Python**: Data extraction and API integration
- **RapidAPI**: API management and rate limiting

### Streaming
- **Apache Kafka**: Real-time data streaming
- **Kafka Producers**: Python-based data producers
- **Kafka Topics**: Organized data streams

### Processing
- **Apache Spark**: Distributed data processing
- **PySpark Structured Streaming**: Real-time stream processing
- **Data Transformations**: Cleaning, enrichment, aggregation

### Storage
- **S3/MinIO**: Data lake storage
- **Parquet**: Columnar storage format
- **PostgreSQL**: Relational database for analytics

### Analytics
- **dbt**: Data transformation and modeling
- **Dimensional Models**: Star schema design
- **Data Quality**: Validation and testing

### Orchestration
- **Apache Airflow**: Workflow orchestration
- **DAGs**: Directed Acyclic Graphs
- **Scheduling**: Automated pipeline execution

### Monitoring
- **Grafana**: Metrics visualization
- **Prometheus**: Metrics collection
- **Alerting**: Real-time notifications

### Visualization
- **Streamlit**: Interactive dashboards
- **Plotly**: Interactive charts
- **Real-time Updates**: Live data visualization

## ğŸ“ Project Structure

```
financial-data-pipeline/
â”œâ”€â”€ data_pipeline/                 # Core data pipeline components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”œâ”€â”€ models.py                  # Data models
â”‚   â”œâ”€â”€ extractors.py              # Data extraction
â”‚   â”œâ”€â”€ transformers.py            # Data transformation
â”‚   â”œâ”€â”€ scheduler.py               # Job scheduling
â”‚   â”œâ”€â”€ monitoring.py              # Monitoring and alerting
â”‚   â””â”€â”€ pipeline.py                # Main orchestrator
â”œâ”€â”€ streaming_pipeline/            # Real-time streaming
â”‚   â”œâ”€â”€ kafka_producer.py          # Kafka data producer
â”‚   â””â”€â”€ pyspark_streaming.py       # PySpark streaming processing
â”œâ”€â”€ data_lake/                     # Data lake storage
â”‚   â””â”€â”€ s3_storage.py              # S3/MinIO integration
â”œâ”€â”€ dbt_models/                    # dbt transformations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Staging models
â”‚   â”‚   â”œâ”€â”€ marts/                 # Mart models
â”‚   â”‚   â””â”€â”€ schema.yml             # Data schema
â”‚   â”œâ”€â”€ dbt_project.yml            # dbt configuration
â”‚   â””â”€â”€ profiles.yml               # Database profiles
â”œâ”€â”€ visualization/                 # Dashboards and visualization
â”‚   â””â”€â”€ dashboard.py               # Streamlit dashboard
â”œâ”€â”€ orchestration/                 # Workflow orchestration
â”‚   â””â”€â”€ airflow_dag.py             # Airflow DAGs
â”œâ”€â”€ monitoring/                    # Monitoring configuration
â”‚   â”œâ”€â”€ grafana/                   # Grafana dashboards
â”‚   â””â”€â”€ prometheus/                # Prometheus config
â”œâ”€â”€ docker-compose.yml             # Container orchestration
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README_COMPLETE_PIPELINE.md    # This file
```

## ğŸ› ï¸ Setup and Installation

### Prerequisites
- Docker and Docker Compose
- Python 3.9+
- Git

### 1. Clone the Repository
```bash
git clone <repository-url>
cd financial-data-pipeline
```

### 2. Environment Setup
```bash
# Create environment file
cp .env.example .env

# Edit environment variables
nano .env
```

### 3. Start the Infrastructure
```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps
```

### 4. Initialize Data Pipeline
```bash
# Install Python dependencies
pip install -r requirements.txt

# Run initial data extraction
python simple_pipeline_example.py
```

## ğŸ”§ Configuration

### Environment Variables
```bash
# API Configuration
RAPIDAPI_KEY=your_rapidapi_key
YAHOO_API_BASE_URL=https://yahoo-finance15.p.rapidapi.com

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=financial_data
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password

# S3/MinIO Configuration
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_BUCKET=financial-data

# Airflow Configuration
AIRFLOW_HOME=/opt/airflow
```

### Service URLs
- **Airflow**: http://localhost:8080 (admin/admin)
- **Streamlit Dashboard**: http://localhost:8501
- **Grafana**: http://localhost:3000 (admin/admin)
- **Jupyter**: http://localhost:8888
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Kafka UI**: http://localhost:8080 (via Airflow)

## ğŸ“Š Data Flow

### 1. Data Ingestion
```python
# Extract data from Yahoo Finance API
extractor = StockQuotesExtractor(rapidapi_key)
job = extractor.extract(symbols=["AAPL", "MSFT", "GOOGL"])
```

### 2. Real-time Streaming
```python
# Stream data to Kafka
producer = YahooFinanceKafkaProducer(rapidapi_key)
producer.produce_stock_quotes(symbols, interval=30)
```

### 3. Stream Processing
```python
# Process streams with PySpark
processor = FinancialDataStreamProcessor()
processor.start_all_streams()
```

### 4. Data Storage
```python
# Store in data lake
storage = S3DataLakeStorage()
storage.upload_parquet_data(data, "stock_quotes")
```

### 5. Analytics
```bash
# Run dbt transformations
dbt run --profiles-dir .
dbt test --profiles-dir .
```

## ğŸ¯ Usage Examples

### Basic Data Extraction
```python
from data_pipeline.pipeline import DataPipeline

# Initialize pipeline
pipeline = DataPipeline()

# Extract specific data
result = pipeline.run_data_source("stock_quotes", symbols=["AAPL", "MSFT"])
print(f"Extracted {result['processed_items']} quotes")
```

### Real-time Streaming
```python
from streaming_pipeline.kafka_producer import YahooFinanceKafkaProducer

# Start streaming
producer = YahooFinanceKafkaProducer()
producer.produce_all_data(symbols=["AAPL", "MSFT", "GOOGL"])
```

### Data Transformation
```python
from data_pipeline.transformers import DataTransformer

# Transform data
transformer = DataTransformer()
result = transformer.transform_data(data, "stock_quotes")
```

### Dashboard Access
```bash
# Start Streamlit dashboard
streamlit run visualization/dashboard.py
```

## ğŸ“ˆ Monitoring and Alerting

### Metrics Collected
- **Data Quality**: Completeness, validity, consistency
- **Processing Performance**: Throughput, latency, error rates
- **System Resources**: CPU, memory, disk usage
- **API Usage**: Request counts, rate limits, errors

### Alerts Configured
- **Data Quality Issues**: Missing data, invalid values
- **Processing Failures**: Job failures, timeouts
- **System Issues**: High resource usage, disk space
- **API Issues**: Rate limiting, authentication errors

### Monitoring Dashboards
- **Pipeline Health**: Overall system status
- **Data Quality**: Quality metrics and trends
- **Performance**: Processing times and throughput
- **Business Metrics**: Market data insights

## ğŸ” Data Quality

### Validation Rules
- **Completeness**: Required fields present
- **Validity**: Data types and ranges correct
- **Consistency**: Cross-field validation
- **Accuracy**: Business rule validation

### Quality Checks
- **Automated Testing**: dbt tests
- **Data Profiling**: Statistical analysis
- **Anomaly Detection**: Outlier identification
- **Trend Analysis**: Quality over time

## ğŸš€ Deployment

### Development
```bash
# Start development environment
docker-compose -f docker-compose.yml up -d

# Run tests
pytest tests/

# Run data quality checks
dbt test
```

### Production
```bash
# Deploy to production
docker-compose -f docker-compose.prod.yml up -d

# Monitor deployment
kubectl get pods -n financial-data
```

### Scaling
- **Horizontal Scaling**: Add more Kafka partitions
- **Vertical Scaling**: Increase container resources
- **Auto-scaling**: Kubernetes HPA configuration

## ğŸ”§ Troubleshooting

### Common Issues
1. **API Rate Limiting**: Check RapidAPI quotas
2. **Kafka Connection**: Verify Kafka is running
3. **Database Connection**: Check PostgreSQL status
4. **S3 Access**: Verify MinIO credentials

### Debug Commands
```bash
# Check service logs
docker-compose logs kafka
docker-compose logs postgres
docker-compose logs airflow-webserver

# Test API connection
python -c "from data_pipeline.extractors import StockQuotesExtractor; print('API OK')"

# Test Kafka connection
python -c "from kafka import KafkaProducer; print('Kafka OK')"
```

## ğŸ“š Documentation

### API Documentation
- **Yahoo Finance API**: [RapidAPI Documentation](https://rapidapi.com/hub)
- **Kafka API**: [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- **dbt Documentation**: [dbt Documentation](https://docs.getdbt.com/)

### Architecture Decisions
- **Why Kafka?**: High-throughput, fault-tolerant streaming
- **Why PySpark?**: Distributed processing, real-time capabilities
- **Why S3?**: Scalable, cost-effective storage
- **Why dbt?**: Data transformation, testing, documentation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For questions or issues:
1. Check the troubleshooting section
2. Review the logs
3. Open an issue on GitHub
4. Contact the data engineering team

## ğŸ”„ Version History

- **v1.0.0**: Initial release with basic pipeline
- **v1.1.0**: Added real-time streaming
- **v1.2.0**: Added monitoring and alerting
- **v1.3.0**: Added dbt transformations
- **v1.4.0**: Added visualization dashboard
- **v1.5.0**: Added container orchestration

---

**Built with â¤ï¸ by the Data Engineering Team**

